<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LLMs in Education: Novel Perspectives, Challenges, and Opportunities">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COLING 2025 Tutorial: LLMs in Education: Novel Perspectives, Challenges, and Opportunities</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/bulma-slider.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>
<body>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">LLMs for Reading Assistance</h2>
        <!-- <p>This is a comprehensive reading list  <br />   -->
  <h3 class="title is-5">Essential Readings</h3>
           <ul style='margin-top:0cm' start=1 type=1>
 <li> <a href="https://aclanthology.org/2023.emnlp-main.821/">BLESS: Benchmarking Large Language Models on
     Sentence Simplification</a> (Kew et.al., 2023)</li>
 <li><a href="https://link.springer.com/article/10.1007/s40593-023-00374-x">Review on Neural Question Generation for
     Education Purposes</a> (Al Farabi et.al., 2023)</li>
   <li><a href="http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.574.pdf">Trends,
     limitations and open challenges in automatic readability assessment
     research</a> (Vajjala, 2022)</li>
 <li><a href="https://aclanthology.org/2021.findings-acl.233.pdf">Automatic text
     simplification for social good: Progress and challenges</a> (Stajner, 2021)</li>
 <li><a href="https://aclanthology.org/Q15-1021/">Problems
     in Current Text Simplification Research: New Data Can Help</a> (Xu et.al., 2015)</li>
</ul>

  <br />
        <h3 class="title is-5">Task: Automatic Readability Assessment</h3>
        <h4><b>Surveys</b></h4>
            <ul>
              <li><a
     href="https://www.jbe-platform.com/content/journals/10.1075/itl.165.2.01col">Computational
     assessment of text readability: A survey of current and future research</a> (Collins-Thompson, 2014)</li>
            </ul>
        <br />
        <h4><b>Methods</b></h4>
        <ul>
<li><a href="https://arxiv.org/abs/2411.01706">Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups
</a>(Smadu et.al., 2024)</li>
<li><a href="https://aclanthology.org/2024.emnlp-main.682">README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment</a>(Naous et.al., 2024)</li>
 <li><a href="https://aclanthology.org/2024.naacl-long.16/">FPT: Feature Prompt Tuning for Few-shot Readability Assessment</a> (Wang et.al., 2024)</li>
 <li><a href="https://aclanthology.org/2024.bea-1.5/">Beyond Flesch-Kincaid:Prompt-based Metrics Improve Difficulty Classification of Educational
     Texts</a> (Rooein et.al., 2024)</li>
      <li><a href="https://aclanthology.org/2024.findings-eacl.153.pdf">Exploring hybrid approaches to readability: experiments on the complementarity
 between linguistic features and transformers</a>(Wilkens et.al., 2024) </li>
 <li><a href="https://aclanthology.org/2024.tsar-1.8/">Difficult for Whom? A Study of Japanese Lexical Complexity</a>(Nohejl et.al., 2024)</li>
   <li><a href="https://aclanthology.org/2023.findings-eacl.135/">Prompt-based Learning for Text Readability Assessment</a> (Lee and Lee, 2023)</li>

 <li><a href="https://aclanthology.org/Q19-1028/">Multiattentive Recurrent Neural Network Architecture for Multilingual Readability
     Assessment</a> (Azpiazu and Pera, 2019)</li>
 <li><a href="https://aclanthology.org/W16-0502.pdf">Text readability assessment for second language learners</a> (Xia et.al., 2016) </li>
</ul>

        <br />
        <h4><b>Evaluation</b></h4>
<ul>
   <li><a href="https://arxiv.org/abs/2410.04484">Fine-Grained Prediction of Reading Comprehension from Eye Movements</a> (Shubi et.al., 2024)</li>
 <li><a href="https://aclanthology.org/W19-4437.pdf">On understanding the relation between expert annotations of text readability and target reader
     comprehension</a> (Vajjala and Lucic, 2019)</li>
 <li><a href="https://aclanthology.org/W16-4105.pdf">Towards grounding computational linguistic approaches to readability: Modeling reader-text
     interaction for easy and difficult texts</a> (Vajjala et.al., 2016)</li>
 <li><a href="https://aclanthology.org/C16-1094.pdf">Are Cohesive Features Relevant for Text Readability Evaluation?</a>(Todirascu et.al., 2016)</li>
</ul>

        <br />
        <h4><b>Shared Tasks</b></h4>
<ul>
 <li><a href="https://aclanthology.org/S16-1085/">SemEval-2016 Task 11: Complex Word Identification</a> (Paetzold and Specia, 2016)</li>
 <li><a href="https://aclanthology.org/2021.semeval-1.1/">SemEval-2021 Task 1: Lexical Complexity Prediction</a>(Shardlow et.al., 2021)</li>
</ul>

        <br />
        <h4><b>Corpora/Other Resources</b></h4>
<ul>
 <li>English: <a href="https://github.com/nishkalavallabhi/OneStopEnglishCorpus"> OneStopEnglish (Vajjala and Lucic, 2018)</a>, <a href="https://github.com/scrosseye/CLEAR-Corpus">CommonLit Ease of Readability Corpus (Crossley et.al., 2021)</a></li>
 <li>German: <a href="https://paperswithcode.com/dataset/textcomplexityde">TextComplexityDE</a> (Naderi et.al., 2019)</li>
</ul>

<br />
<h4><b>Explainability/Interpretability</b></h4>
<ul>
 <li><a href="https://aclanthology.org/2024.bea-1.17/">Explainable AI in Language
     Learning: Linking Empirical Evidence and Theoretical Concepts in
     Proficiency and Readability Modeling of Portuguese</a> (Ribeiro-Flucht et.al., 2024)</li>
 <li><a href="https://aclanthology.org/2023.bea-1.42/">“Geenmakkie</span>”: Interpretable Classification and
     Simplification of Dutch Text Complexity</a> (Hobo et.al., 2023)</li>
</ul>

<br />
        <h3 class="title is-5">Task: Automatic Text Simplification</h3>
         <h4><b>Surveys</b></h4>
        <ul>
          <li><a href="https://direct.mit.edu/coli/article/46/1/135/93384/Data-Driven-Sentence-Simplification-Survey-and">Data-Driven
          Sentence Simplification: Survey and Benchmark</a>(Alva-Manchego et.al., 2020)</li>
          <li><a href="https://www.jbe-platform.com/content/journals/10.1075/itl.165.2.06sid">A
              survey of research on text simplification</a> (Siddharthan, 2014)</li>
        </ul>

<br />
<h4><b>Methods:</b></h4>

<ul>
    <li><a href="https://aclanthology.org/2024.findings-emnlp.530/">ExpertEase: A Multi-Agent Framework for Grade-Specific Document
    Simplification with Large Language Models</a> (Mo and Hu, 2024)</li>
    <li><a href="https://ceur-ws.org/Vol-3596/paper37.pdf">Is It Really That Simple? Prompting Language Models for
Automatic Text Simplification in Italian</a> (Noza and Attanasio, 2024)</li>
    <li><a href="https://aclanthology.org/2024.lrec-main.815">Is it Possible to Modify Text to a Target Readability Level? An Initial
Investigation Using Zero-Shot Large Language Models</a> (Farajidizaji et.al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.emnlp-main.357/">Evaluating LLMs for Targeted Concept Simplification forDomain-Specific Texts
    </a> (Asthana et.al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.findings-emnlp.419/">Edit-Constrained Decoding for Sentence Simplification</a> (Zetsu et.al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.emnlp-main.999/">Label Confidence Weighted Learning for Target-level Sentence Simplification</a> (Qiu and Zhang, 2024)</li>
    <li><a href="https://aclanthology.org/2024.sigdial-1.3/">Elaborative Simplification for German-language Texts</a>(Hewett et.al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.lrec-main.102/">An LLM-Enhanced Adversarial Editing System for Lexical Simplification</a>(Tan et.al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.cl4health-1.12/">On Simplification of Discharge Summaries in Serbian: Facing the Challenges</a>(Zecevic et.al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.findings-acl.952/">DIMSIM: Distilled Multilingual Critics for Indic Text Simplification</a>(Mondal et.al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.findings-acl.895/">Enhancing Sentence Simplification in Portuguese: Leveraging Paraphrases, Context, and Linguistic Features</a>(Scalercio et.al., 2024)</li>
    <li><a href="https://arxiv.org/abs/2409.20246">Analysing Zero-Shot Readability-Controlled Sentence Simplification</a>(Barayan et.al., 2024)</li>
    <li><a href="https://arxiv.org/abs/2410.14028">Measuring and Modifying the Readability of English Texts with GPT-4</a> (Trott et.al., 2024)</li>
    <li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00606/118113">Learning to Paraphrase Sentences to Different Complexity Levels</a>(Chi et.al., 2023)</li>
    <li><a href="https://arxiv.org/abs/2005.00352">MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases</a>(Martin et.al., 2022)</li>
    <li><a href="https://aclanthology.org/2021.acl-long.498/">Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text</a>(Laban et.al., 2021)</li>
    <li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00107/43364/Optimizing-Statistical-Machine-Translation-for">Optimizing statistical machine translation for text simplification</a>(Xu et.al., 2016)</li>
    <li><a href="https://aclanthology.org/D11-1038.pdf">Learning to simplify sentences with quasi-synchronous grammar and integer programming</a>(Woodsend and Lapata, 2011)</li>
    <li><a href="https://www.sciencedirect.com/science/article/pii/S0950705197000294">Automatic induction of rules for text simplification</a>(Chandrasekar and Srinivas, 1997)</li>
</ul>

<br />
<h4><b>Evaluation: </b></h4>

<ul>
   <li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00653/120649/Do-Text-Simplification-Systems-Preserve-Meaning-A">Do Text Simplification Systems Preserve Meaning? A Human Evaluation via
     Reading Comprehension</a> (Agrawal and Carpuat, 2024)</li>
 <li><a href="https://arxiv.org/pdf/2403.17640">REFeREE: A REference-FREE Model-Based Metric for Text Simplification</a></li> (Huang and Kochmar, 2024)
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3613904.3642570">Digital Comprehensibility Assessment of Simplified Texts among Persons with
     Intellectual Disabilities</a></li> (Säuberli et.al., 2024)
 <li><a href="https://aclanthology.org/2022.emnlp-main.121">Linguistic Corpus Annotation for Automatic Text Simplification Evaluation</a> (Cardon et.al., 2022)</li>
 <li><a href="https://direct.mit.edu/coli/article/47/4/861/106930/The-Un-Suitability-of-Automatic-Evaluation-Metrics">The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification</a> (Alva-Manchego et.al., 2021)</li>
 <li><a href="https://aclanthology.org/2021.findings-acl.77.pdf">Investigating Text Simplification Evaluation</a> (Vásquez-Rodríguez et.al., 2021)</li>
</ul>

<br />
<h4><b>Shared Tasks</b></h4>

<ul>
 <li><a href="https://aclanthology.org/2024.bea-1.51/">The BEA 2024 Shared Task on the Multilingual Lexical Simplification Pipeline</a></li>
 <li><a href="https://aclanthology.org/S12-1046/">SemEval-2012 Task 1: English Lexical Simplification</a></li>
</ul>

<br />
<h4><b>Corpora/Other Resources</b></h4>

<ul>
  <li><a href="https://aclanthology.org/2024.tsar-1.9/">Lexical Complexity Prediction and Lexical Simplification for Catalan and Spanish: Resource Creation, Quality Assessment, and Ethical Considerations</a>(Saggion et.al., 2024)</li>
 <li><a href="https://arxiv.org/abs/2409.20466">Language Resources in Spanish for Automatic Text Simplification across Domains</a> (Moreno-Sandoval et.al., 2024)</li>
 <li><a href="https://jantrienes.com/text-simplification-datasets/">Text simplification datasets listing</a> (Jan Trienes, 2024)</li>
 <li><a href="https://aclanthology.org/2024.tsar-1.3/">CompLex-ZH: A New Dataset for Lexical Complexity Prediction in Mandarin and Cantonese</a> (Qiu et.al., 2024)</li>
</ul>

<h4><b>Other Related Topics</b></h4>
<ul>
  <li><a href="https://aclanthology.org/2024.tsar-1.6/">Considering Human Interaction and Variability in Automatic Text Simplification</a>(Kim et.al., 2024)</li>
<li><a href="https://aclanthology.org/2024.findings-emnlp.877">ARTS: Assessing Readability & Text Simplicity 🎨</a>(Engelmann et.al., 2024)</li>
</ul>


<br />

<h3 class="title is-5">Task: Automatic Question Generation</h3>
<h4><b>Surveys</b></h4>

<ul>
 <li><a href="https://link.springer.com/article/10.1007/s40593-019-00186-y">A systematic review of automatic question
     generation for educational purposes </a> (Kurdi et.al., 2020)</li>
</ul>

  <br />      
<h4><b>Methods</b></h4>

<ul>
  <li><a href="https://drive.google.com/file/d/1MYsQn9uSmXxtkWdTccZtTSOkEgmUH1vb/view">Automatic Question Generation and Constructed Response Scoring in Intelligent Texts</a>(Morris et.al., 2024)</li>
  <li><a href="https://www.sciencedirect.com/science/article/pii/S2666920X24001012">Analysis of LLMs for educational question classification and generation</a>(Faraby et.al., 2024)</li>
  <li><a href="https://arxiv.org/abs/2410.12893">MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation
</a> (Deroy et.al., 2024)</li>
<li><a href="https://aclanthology.org/2024.readi-1.3/">Automatic Generation and Evaluation of Reading Comprehension
Test Items with Large Language Models</a>(Sauberli and Clematide, 2024)</li>
   <li><a href="https://aclanthology.org/2024.bea-1.10/">Improving Socratic Question Generation using Data Augmentation and Preference Optimization</a> (Kumar and Lan, 2024)</li>
 <li><a href="https://aclanthology.org/2024.bea-1.19/">Improving Automated Distractor Generation for Math Multiple-choice Questions with Overgenerate-and-rank</a> (Scarlatos et.al., 2024)</li>
 <li><a href="https://aclanthology.org/2023.bea-1.10/">Difficulty-Controllable Neural Question Generation for Reading Comprehension using Item Response Theory</a> (Uto et.al., 2023)</li>
 <li><a href="https://aclanthology.org/2023.bea-1.47/">Comparing Neural Question Generation Architectures for Reading Comprehension</a> (Perkoff et.al., 2023)</li>
 <li><a href="https://arxiv.org/abs/2410.03197">Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages </a>(Hwang et.al., 2023)</li>
</ul>

  <br />      
<h4><b>Evaluation</b></h4>

<ul>
<li><a href=https://raw.githubusercontent.com/mlresearch/v257/main/assets/gorgun24a/gorgun24a.pdf"">Current Evaluation Methods are a Bottleneck in Automatic Question Generation</a>(Gorgun and Bulut, 2024)</li>
 <li><a href="https://aclanthology.org/2024.bea-1.1/">How Good are Modern LLMs in Generating Relevant and High-Quality Questions at Different Bloom’s Skill
     Levels for Indian High School Social Science Curriculum?</a> (Scaria et.al., 2024)</li>
 <li><a href="https://aclanthology.org/2023.bea-1.52/">Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase of ChatGPT in Education Applications</a> (Xiao et.al., 2024)</li>
</ul>

<br />
<h3 class="title is-5">Task: Other Applications</h3>
<ul>
<li><a href="https://arxiv.org/abs/2304.04616">Automated Reading Passage Generation with OpenAI's Large Language Model
</a> (Bezirhan and Davier, 2024)</li>
<li><a href="https://arxiv.org/abs/2411.11520">A Pre-Trained Graph-Based Model for Adaptive Sequencing of Educational Documents
</a> (Vassoyan et.al., 2024)</li>
<li><a href="https://arxiv.org/abs/2402.12593">Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation</a>(Imperial et.al., 2024)</li>
 <li><a href="https://arxiv.org/abs/2403.19506">LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae</a></li>
</ul>
      </div>
    </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      Add citation
}
</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/ACL2023-Retrieval-LM" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website <a href="https://github.com/nerfies/nerfies.github.io">source code.</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
